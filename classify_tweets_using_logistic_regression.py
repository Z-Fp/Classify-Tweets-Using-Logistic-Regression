# -*- coding: utf-8 -*-
"""Classify Tweets Using Logistic Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1on68Azrg3a6I4PWNttmbwGfbRM9DBquI
"""

import nltk                                  
from nltk.corpus import twitter_samples, stopwords
import matplotlib.pyplot as plt              
import numpy as np       
import keras
from utils import process_tweet, build_freqs

"""**Prepare Data**"""

nltk.download('twitter_samples')
nltk.download('stopwords')
print(stopwords.words('english'))

"""**List of Tweets**"""

# select the lists of positive and negative tweets
all_positive_tweets = twitter_samples.strings('positive_tweets.json')
all_negative_tweets = twitter_samples.strings('negative_tweets.json')

# concatenate the lists, 1st part is the positive tweets followed by the negative
tweets = all_positive_tweets + all_negative_tweets

# let's see how many tweets we have
print("Number of tweets: ", len(tweets))

print(all_positive_tweets[10])
print(all_negative_tweets[1543])

"""**Labels**"""

labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))

"""**build_freqs function**

count frequency of each word

"""

freqs = build_freqs(tweets, labels)

print(freqs[('great', 1)], freqs[('great', 0)])
freqs.get(('excellent', 1), 0)

"""**extractFeatures**"""

def extractFeatures(tweet):
  tweet = process_tweet(tweet)
  pos, neg = 0, 0
  for word in tweet:
    pos += freqs.get((word, 1.), 0)
    neg += freqs.get((word, 0.), 0)
  feature_vec = np.array([pos, neg])
  return feature_vec

"""Spliting Data"""

train_tweets = all_positive_tweets[:4000] + all_negative_tweets[:4000]
train_labels = [1. for i in range(4000)] + [0. for i in range(4000)]
test_tweets = all_positive_tweets[4000:] + all_negative_tweets[4000:]
test_labels = [1. for i in range(1000)] + [0. for i in range(1000)]


X_train = np.zeros((len(train_tweets), 2))
y_train = np.array(train_labels)
X_test = np.zeros((len(test_tweets), 2))
y_test = np.array(test_labels)


for i in range(X_train.shape[0]):
  X_train[i] = extractFeatures(train_tweets[i])

for i in range(X_test.shape[0]):
  X_test[i] = extractFeatures(test_tweets[i])

"""**Logistic Regression**

"""

model = keras.Sequential([keras.layers.Dense(1, activation='sigmoid', input_shape=(2,))])
model.summary()

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))

print(all_negative_tweets[-1])
print(X_test[-1])
print(model.predict(X_test[-1:]))

"""**classify**"""

def classify(tweet):
  tweet = extractFeatures(tweet)
  return model.predict(tweet[np.newaxis, :])

neg = 'I hated it. It was such an awful movie!'
pos = 'It was an honor to view this great one!'
print(classify(neg), classify(pos))